# processsing.py
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
import os
from datetime import datetime
import re
import numpy as np


# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤
_CURRENCY_RATES_CACHE = None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
PROCESSING_STATS = {}


def reset_stats():
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    global PROCESSING_STATS
    PROCESSING_STATS = {
        'original_rows': 0,
        'original_cols': 0,
        'removed_duplicates': 0,
        'removed_cancelled': 0,
        'removed_empty_voucher': 0,
        'filled_buyer_name_client_hall': 0,
        'filled_buyer_name_undefined': 0,
        'converted_currency': 0,
        'extracted_regions': 0,
        'final_rows': 0,
        'final_cols': 0,
        'added_cols': []
    }


def get_currency_rates(rates_file='/home/vulcan4ik/dashboard-cruise-app/app_data/currency_rates_2024-2025.csv'):

    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _CURRENCY_RATES_CACHE

    if _CURRENCY_RATES_CACHE is not None:
        return _CURRENCY_RATES_CACHE

    try:
        # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –ü–†–û–í–ï–†–ö–£:
        if not os.path.exists(rates_file):
            print(f"‚ùå –§–∞–π–ª –∫—É—Ä—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {rates_file}")
            return None

        rates_df = pd.read_csv(rates_file)
        rates_df['date'] = pd.to_datetime(rates_df['date'])
        _CURRENCY_RATES_CACHE = rates_df
        print(f"‚úÖ –ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(rates_df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥ –∫—É—Ä—Å–æ–≤: {rates_df['date'].min().date()} - {rates_df['date'].max().date()}")
        return rates_df
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç: {e}")
        return None


def convert_to_rub(row, rates_df):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—É–º–º—É –≤ —Ä—É–±–ª–∏ —Å —É—á–µ—Ç–æ–º –∫—É—Ä—Å–∞ –¶–ë + 4.5%"""
    global PROCESSING_STATS

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å—É–º–º—ã
    if pd.isna(row.get('amount_to_pay')) or row['amount_to_pay'] in [0, '', None]:
        return 0

    amount = float(row['amount_to_pay'])
    currency = str(row.get('currency', '—Ä–±')).strip()

    # –ú–∞–ø–ø–∏–Ω–≥ –≤–∞–ª—é—Ç
    currency_map = {
        'E': 'EUR',
        'EUR': 'EUR',
        '‚Ç¨': 'EUR',
        '$': 'USD',
        'USD': 'USD',
        '—Ä–±': 'RUB',
        'RUB': 'RUB',
        '—Ä—É–±': 'RUB'
    }

    target_currency = currency_map.get(currency, 'RUB')

    # –ï—Å–ª–∏ —Ä—É–±–ª–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    if target_currency == 'RUB':
        return round(amount, 2)

    # –î–ª—è –≤–∞–ª—é—Ç—ã –Ω—É–∂–Ω–∞ –¥–∞—Ç–∞ –∏ –∫—É—Ä—Å
    creation_date = row.get('creation_date')

    if rates_df is None or pd.isna(creation_date):
        print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: –≤–∞–ª—é—Ç–∞={currency}, –¥–∞—Ç–∞={creation_date}")
        return 0

    try:
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é –ø—Ä–µ–¥—ã–¥—É—â—É—é –¥–∞—Ç—É —Å –∫—É—Ä—Å–æ–º
        creation_date = pd.to_datetime(creation_date)
        available_dates = rates_df[rates_df['date'] <= creation_date]

        if available_dates.empty:
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ —Ä–∞–Ω—å—à–µ –≤—Å–µ—Ö –∫—É—Ä—Å–æ–≤ - –±–µ—Ä–µ–º —Å–∞–º—ã–π —Ä–∞–Ω–Ω–∏–π –∫—É—Ä—Å
            rate_row = rates_df.iloc[0]
            print(f"‚ö†Ô∏è –î–∞—Ç–∞ {creation_date.date()} —Ä–∞–Ω—å—à–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º {rate_row['date'].date()}")
        else:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫—É—Ä—Å –¥–æ –¥–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è
            rate_row = available_dates.iloc[-1]

        if target_currency in rate_row.index and pd.notna(rate_row[target_currency]):
            rate = float(rate_row[target_currency])
            # –ö—É—Ä—Å –¶–ë + 4.5% –Ω–∞—Ü–µ–Ω–∫–∞
            converted_amount = (amount * rate) * 1.045
            PROCESSING_STATS['converted_currency'] += 1
            return round(converted_amount, 2)
        else:
            print(f"‚ùå –í–∞–ª—é—Ç–∞ {target_currency} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—É—Ä—Å–∞—Ö")
            return 0

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–ª—è —Å—Ç—Ä–æ–∫–∏: {e}")
        return 0


def clean_numeric_data(df):
    """–û—á–∏—â–∞–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –∑–∞–ø—è—Ç—ã—Ö –∏ –¥—Ä—É–≥–∏—Ö –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""

    def convert_numeric_value(x):
        if pd.isna(x) or x == '':
            return x

        str_val = str(x).strip()

        # –£–±–∏—Ä–∞–µ–º –∑–∞–ø—è—Ç—ã–µ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á)
        cleaned = str_val.replace(',', '')

        try:
            return float(cleaned)
        except ValueError:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return x

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º —Å—Ç–æ–ª–±—Ü–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ —á–∏—Å–ª–æ–≤—ã–µ
    for column in df.columns:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —Å—Ç–æ–ª–±—Ü–µ —Å—Ç—Ä–æ–∫–∏ —Å –∑–∞–ø—è—Ç—ã–º–∏
        has_commas = df[column].apply(lambda x: ',' in str(x) if pd.notna(x) else False).any()

        if has_commas:
            df[column] = df[column].apply(convert_numeric_value)

    return df


def process_data(file_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    global PROCESSING_STATS
    reset_stats()

    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
    else:
        df = pd.read_excel(file_path)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    PROCESSING_STATS['original_rows'] = len(df)
    PROCESSING_STATS['original_cols'] = len(df.columns)

    print(f"üìÇ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")

    # –ü–ï–†–í–´–ú –î–ï–õ–û–ú - –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
    df = rename_columns(df)

    # –ó–∞—Ç–µ–º –æ—á–∏—â–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = clean_numeric_data(df)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
    df = clean_data(df)
    df = fill_missing_buyer_names(df)
    df = enrich_data(df)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    PROCESSING_STATS['final_rows'] = len(df)
    PROCESSING_STATS['final_cols'] = len(df.columns)
    PROCESSING_STATS['added_cols'] = ['amount_rub', 'region', 'is_cruise_seller', 'payment_percentage', 'days_until_checkin', 'creation_month']

    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")

    return df


def rename_columns(df):
    """–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–æ–≤–∞—Ä—é"""
    rename_dict = {
        '–ü—É—Ç–µ–≤–∫–∞': 'voucher_id',
        '–°—Ç—Ä–∞–Ω–∞': 'country',
        '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è': 'creation_date',
        '–î–∞—Ç–∞ –∑–∞–µ–∑–¥–∞': 'checkin_date',
        '–î–Ω–µ–π': 'days',
        '–ß–µ–ª–æ–≤–µ–∫': 'people',
        '–°—Ç–∞—Ç—É—Å –ø—É—Ç–µ–≤–∫–∏': 'voucher_status',
        '–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å—Ç–∞—Ç—É—Å': 'internal_status',
        '–í–∞–ª—é—Ç–∞': 'currency',
        '–°—É–º–º–∞ –∫ –æ–ø–ª–∞—Ç–µ': 'amount_to_pay',
        '–û–ø–ª–∞—Ç–∞': 'payment',
        '–ù–∞–∑–≤–∞–Ω–∏–µ —Ç—É—Ä–∞': 'tour_name',
        '–ü–æ–∫—É–ø–∞—Ç–µ–ª—å: –û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ': 'buyer_department',
        '–ü–æ–∫—É–ø–∞—Ç–µ–ª—å: –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': 'buyer_name',
        '–ü–æ–∫—É–ø–∞—Ç–µ–ª—å: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¢–ê': 'buyer_category',
        '–°–æ–∑–¥–∞—Ç–µ–ª—å': 'creator',
        '–í–µ–¥—É—â–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä': 'manager'
    }

    # –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º - pandas –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
    df = df.rename(columns=rename_dict)

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è (—Ç–µ –∫–æ—Ç–æ—Ä—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è)
    final_columns = [v for v in rename_dict.values() if v in df.columns]
    df = df[final_columns]

    print(f"‚úÖ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤: {len(final_columns)}")
    print(f"üìã –ò—Ç–æ–≥–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {list(df.columns)}")

    return df


def clean_data(df):
    """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    global PROCESSING_STATS

    print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —É–¥–∞–ª–µ–Ω–Ω—ã–º–∏/–∞–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—É—Ç–µ–≤–∫–∞–º–∏
    initial_count = len(df)
    df = df[~df['voucher_status'].isin(['–£–¥–∞–ª–µ–Ω', '–ê–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω', '—É–¥–∞–ª–µ–Ω', '–∞–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω'])]
    PROCESSING_STATS['removed_cancelled'] = initial_count - len(df)
    print(f"üîç –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∞–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö/—É–¥–∞–ª–µ–Ω–Ω—ã—Ö: {df.shape}")

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º voucher_id (–≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏)
    if 'voucher_id' in df.columns:
        initial_count = len(df)
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –Ω–µ–ø—É—Å—Ç—ã—Ö voucher_id
        non_empty_mask = (
            df['voucher_id'].notna() &
            (df['voucher_id'] != '') &
            (df['voucher_id'].astype(str).str.strip() != '')
        )
        df = df[non_empty_mask]
        removed_count = initial_count - len(df)
        PROCESSING_STATS['removed_empty_voucher'] = removed_count
        if removed_count > 0:
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º voucher_id: {removed_count}")

    print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
    return df


def fill_missing_buyer_names(df):
    """–ó–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –≤ buyer_name: –ö–õ–ò–ï–ù–¢–°–ö–ò–ô –ó–ê–õ –∏–ª–∏ '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'"""
    global PROCESSING_STATS

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
    if 'buyer_department' not in df.columns or 'buyer_name' not in df.columns:
        return df

    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω—è–µ–º –ö–õ–ò–ï–ù–¢–°–ö–ò–ô –ó–ê–õ
    mask_client_hall = (
        (df['buyer_department'] == '–ö–õ–ò–ï–ù–¢–°–ö–ò–ô –ó–ê–õ') &
        (df['buyer_name'].isna() | (df['buyer_name'] == ''))
    )
    count_client_hall = mask_client_hall.sum()
    df.loc[mask_client_hall, 'buyer_name'] = '–ö–õ–ò–ï–ù–¢–°–ö–ò–ô –ó–ê–õ'
    PROCESSING_STATS['filled_buyer_name_client_hall'] = count_client_hall

    # –ó–∞—Ç–µ–º –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏
    mask_other = df['buyer_name'].isna() | (df['buyer_name'] == '')
    count_other = mask_other.sum()
    df.loc[mask_other, 'buyer_name'] = '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'
    PROCESSING_STATS['filled_buyer_name_undefined'] = count_other

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if count_client_hall > 0:
        print(f"‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω–æ '–ö–õ–ò–ï–ù–¢–°–ö–ò–ô –ó–ê–õ': {count_client_hall} –∑–∞–ø–∏—Å–µ–π")
    if count_other > 0:
        print(f"‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω–æ '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω': {count_other} –∑–∞–ø–∏—Å–µ–π")

    return df


def extract_region(agency_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–≥–∏–æ–Ω –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞"""
    if not isinstance(agency_name, str) or agency_name.strip() == '' or agency_name.lower() in ['n/a', 'nan', 'none']:
        return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'

    text = agency_name.strip()

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
    known_cities = {
        '–ú–æ—Å–∫–≤–∞': ['–º–æ—Å–∫–≤–∞', '–º—Å–∫', 'moscow'],
        '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': ['—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥', '—Å–ø–±', '–ø–∏—Ç–µ—Ä', 'st. petersburg', 'petersburg'],
        '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': ['–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '–Ω–æ–≤–æ—Å–∏–±'],
        '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': ['–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', '–µ–∫–±'],
        '–ö–∞–∑–∞–Ω—å': ['–∫–∞–∑–∞–Ω—å', 'kazan'],
        '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä': ['–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä'],
        '–ü–µ—Ä–º—å': ['–ø–µ—Ä–º—å', 'perm'],
        '–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É': ['—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É', '—Ä–æ—Å—Ç–æ–≤'],
        '–¢—é–º–µ–Ω—å': ['—Ç—é–º–µ–Ω—å'],
        '–ë–∞—Ä–Ω–∞—É–ª': ['–±–∞—Ä–Ω–∞—É–ª'],
        '–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫': ['–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫'],
        '–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫': ['–≤–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫', 'vladivostok'],
        '–°–∞–º–∞—Ä–∞': ['—Å–∞–º–∞—Ä–∞', 'samara'],
        '–ú–∏–Ω—Å–∫': ['–º–∏–Ω—Å–∫', 'minsk'],
        '–ë–∏—à–∫–µ–∫': ['–±–∏—à–∫–µ–∫', 'bishkek'],
        '–ê—Å—Ç–∞–Ω–∞': ['–∞—Å—Ç–∞–Ω–∞', 'astana'],
        '–°–æ—á–∏': ['—Å–æ—á–∏', 'sochi'],
        '–Ø—Ä–æ—Å–ª–∞–≤–ª—å': ['—è—Ä–æ—Å–ª–∞–≤–ª—å'],
        '–í–æ—Ä–æ–Ω–µ–∂': ['–≤–æ—Ä–æ–Ω–µ–∂'],
        '–ò—Ä–∫—É—Ç—Å–∫': ['–∏—Ä–∫—É—Ç—Å–∫'],
        '–•–∞–±–∞—Ä–æ–≤—Å–∫': ['—Ö–∞–±–∞—Ä–æ–≤—Å–∫'],
        '–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å': ['—Å—Ç–∞–≤—Ä–æ–ø–æ–ª—å'],
        '–ß–µ–ª—è–±–∏–Ω—Å–∫': ['—á–µ–ª—è–±–∏–Ω—Å–∫'],
        '–ù–æ–≤–æ—Ä–æ—Å—Å–∏–π—Å–∫': ['–Ω–æ–≤–æ—Ä–æ—Å—Å–∏–π—Å–∫'],
        '–¢–æ–º—Å–∫': ['—Ç–æ–º—Å–∫'],
        '–ö–∏–µ–≤': ['–∫–∏–µ–≤', 'kyiv'],
        '–¢–∞—à–∫–µ–Ω—Ç': ['—Ç–∞—à–∫–µ–Ω—Ç', 'tashkent'],
        '–ï—Ä–µ–≤–∞–Ω': ['–µ—Ä–µ–≤–∞–Ω', 'yerevan'],
        '–ë–∞–∫—É': ['–±–∞–∫—É', 'baku'],
        '–ê–ª–º–∞—Ç—ã': ['–∞–ª–º–∞—Ç—ã', 'almaty'],
    }

    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –≥–æ—Ä–æ–¥–∞ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Å—Ç—Ä–æ–∫–∏
    for city, variants in known_cities.items():
        for variant in variants:
            if variant in text.lower():
                return city

    # –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π)
    parts = [p.strip() for p in re.split(r'[,;]', text) if p.strip()]

    if len(parts) > 1:
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å (–æ–±—ã—á–Ω–æ —Ç–∞–º –≥–æ—Ä–æ–¥)
        last_part = parts[-1]

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = {
            '–ò–ü', '–¢–†–ï–í–ï–õ', '–ì–†–£–ü–ü', '–¢–£–†', '–í–û–Ø–ñ', '–ö–û–†–ê–õ', '–ê–ù–ï–ö–°', 'PAC', '–ü–ê–ö',
            'TRAVEL', 'GROUP', '–û–û–û', '–ó–ê–û', '–ê–û', 'LTD', 'CORP', 'COMPANY', 'CLUB',
            '–º.', '—É–ª.', '–ø—Ä.', '–±—É–ª—å–≤–∞—Ä', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '—É–ª–∏—Ü–∞', '–¶–ï–ù–¢–†', '–û–§–ò–°', '–û–¢–î–ï–õ',
            '–§–ò–õ–ò–ê–õ', '–ê–ì–ï–ù–¢–°–¢–í–û', '–ë–Æ–†–û', '–°–ï–¢–¨', '–ö–û–ú–ü–ê–ù–ò–Ø', 'EXPERT', 'EXPERTS',
            'WORLD', 'INTERNATIONAL', 'SERVICE', 'SERVICES', '–ö–†–£–ö–õ–ê–ë', '–ê–õ–õ–ò–ù–¢–†–≠–í–ï–õ',
            '–ì–ï–†–ú–ï–°', '–°–ê–ù–≠–ö–°–ü–†–ï–°–°-–ì–ü', '–ú–ê –ú–ò–õ–¨–Ø–ù–ê', '–ö–†–£–ì–û–ó–û–†', '–ü–†–ê–ô–ú', '–≠–î–ï–ú-–°–ï–†–í–ò–°',
            '–ë–£–¢–ò–ö –ü–£–¢–ï–®–ï–°–¢–í–ò–ô', '–ê–ü –ê–†–§–ê', '–ö–†–ê–°–ö–ò –ú–ò–†–ê', '–ë–û–ù–ñ–£–†', '–ú–ï–†–ò–î–ò–ê–ù',
            '–î–ò–†–ï–ö–¢–û–†–ò–£–ú', '–†–ï–ì–ò–û–ù', '–í–û–õ–ì–ê', '–°–ò–ë–ò–†–¨', '–£–†–ê–õ', '–î–ê–õ–¨–ù–ò–ô –í–û–°–¢–û–ö'
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ –∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        if (not any(stop_word in last_part.upper() for stop_word in stop_words) and
            len(last_part) >= 3 and
            not last_part.isdigit() and
            re.match(r'^[–ê-–Ø–Å–∞-—è—ëA-Za-z\- ]+$', last_part)):

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–±—â–∏—Ö —Å–ª–æ–≤
            common_words = {'–¢–£–†–ò–ó–ú', '–û–¢–î–´–•', '–ü–£–¢–ï–®–ï–°–¢–í–ò–ô', '–¢–£–†–û–í', '–í–û–Ø–ñ', '–¢–†–ï–í–ï–õ'}
            if not any(word in last_part.upper() for word in common_words):
                return last_part.strip()

    return '–î—Ä—É–≥–æ–π'


def enrich_data(df):

    global PROCESSING_STATS

    print(f"üîÑ –ù–∞—á–∞–ª–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")


    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ datetime —Ñ–æ—Ä–º–∞—Ç
    if 'creation_date' in df.columns:
        df['creation_date'] = pd.to_datetime(df['creation_date'], errors='coerce')

    if 'checkin_date' in df.columns:
        df['checkin_date'] = pd.to_datetime(df['checkin_date'], errors='coerce')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—É—Ä—Å—ã –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π
    rates_df = get_currency_rates()

    if rates_df is None:
        print(f"‚ö†Ô∏è  –ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
        df['amount_rub'] = 0
    else:
        # –ø–µ—Ä–µ–¥–∞—ë–º rates_df –≤ —Ñ—É–Ω–∫—Ü–∏—é
        print(f"üí± –ù–∞—á–∞–ª–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤–∞–ª—é—Ç...")
        df['amount_rub'] = df.apply(
            lambda row: convert_to_rub(row, rates_df),
            axis=1
        )
        print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–æ–∫: {PROCESSING_STATS['converted_currency']}")

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ —Å—Ç—Ä–∞–Ω—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if 'country' in df.columns:
        print(f"üåç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–æ–≤...")
        df['region'] = df['country'].apply(extract_region)
        PROCESSING_STATS['extracted_regions'] = df['region'].notna().sum()
    else:
        df['region'] = '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'

    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –∫—Ä—É–∏–∑–Ω–∞—è –ª–∏ –ø—É—Ç–µ–≤–∫–∞
    df['is_cruise_seller'] = False
    if 'tour_name' in df.columns:
        df['is_cruise_seller'] = df['tour_name'].str.contains('–∫—Ä—É–∏–∑|cruise', case=False, na=False)

    # –ü—Ä–æ—Ü–µ–Ω—Ç –æ–ø–ª–∞—Ç—ã
    df['payment_percentage'] = 0.0
    if 'payment' in df.columns and 'amount_rub' in df.columns:
        mask = df['amount_rub'] > 0
        df.loc[mask, 'payment_percentage'] = (
            (df.loc[mask, 'payment'] / df.loc[mask, 'amount_rub'] * 100).round(2)
        )

    # –î–Ω–∏ –¥–æ –∑–∞–µ–∑–¥–∞
    if 'checkin_date' in df.columns:
        df['checkin_date'] = pd.to_datetime(df['checkin_date'], errors='coerce')
        df['days_until_checkin'] = (df['checkin_date'] - pd.Timestamp.now()).dt.days
    else:
        df['days_until_checkin'] = 0

    # –ú–µ—Å—è—Ü —Å–æ–∑–¥–∞–Ω–∏—è
    if 'creation_date' in df.columns:
        df['creation_date'] = pd.to_datetime(df['creation_date'], errors='coerce')
        df['creation_month'] = df['creation_date'].dt.strftime('%Y-%m')
    else:
        df['creation_month'] = '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'

    print(f"‚úÖ –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return df


def upload_to_sheets(df, credentials_file='/home/vulcan4ik/dashboard-cruise-app/credentials.json', spreadsheet_name=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Google Sheets —Å fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    –í–°–ï–ì–î–ê —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç CSV —Ñ–∞–π–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    """
    # –°–ù–ê–ß–ê–õ–ê –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
    csv_filename = save_data_locally(df)

    # –ü–û–¢–û–ú –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ Google Sheets (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("\nüì§ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Sheets...")

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ credentials
        if not os.path.exists(credentials_file):
            print(f"‚ö†Ô∏è –§–∞–π–ª credentials –Ω–µ –Ω–∞–π–¥–µ–Ω: {credentials_file}")
            print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤ Google Sheets")
            return csv_filename

        print(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è credentials —Ñ–∞–π–ª: {credentials_file}")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Google Sheets
        scope = ['https://spreadsheets.google.com/feeds',
                 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_file(credentials_file, scopes=scope)
        client = gspread.authorize(creds)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –∏–º—è —Ç–∞–±–ª–∏—Ü—ã
        SPREADSHEET_NAME = "Cruise_Analytics_Dashboard"

        try:
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–∫—Ä—ã—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ç–∞–±–ª–∏—Ü—É
            spreadsheet = client.open(SPREADSHEET_NAME)
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ç–∞–±–ª–∏—Ü—É: {SPREADSHEET_NAME}")
        except gspread.SpreadsheetNotFound:
            # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            spreadsheet = client.create(SPREADSHEET_NAME)
            print(f"üìä –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {SPREADSHEET_NAME}")

        worksheet = spreadsheet.get_worksheet(0)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = [df.columns.tolist()] + df.values.tolist()

        # –û—á–∏—â–∞–µ–º –ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        worksheet.clear()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        worksheet.update('A1', data)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø
        spreadsheet.share(None, perm_type='anyone', role='reader')

        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Google Sheets!")
        print(f"üìä –°—Å—ã–ª–∫–∞: {spreadsheet.url}")

        # –í–ê–ñ–ù–û: –í—Å—ë —Ä–∞–≤–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º—è CSV —Ñ–∞–π–ª–∞
        return csv_filename

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Sheets: {str(e)}")
        print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Å –ª–æ–∫–∞–ª—å–Ω—ã–º CSV —Ñ–∞–π–ª–æ–º")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
        return csv_filename


def save_data_locally(df):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω–æ"""
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        results_dir = '/home/vulcan4ik/dashboard-cruise-app/results'
        os.makedirs(results_dir, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º DataFrame
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"processed_{timestamp}.csv"
        csv_path = os.path.join(results_dir, csv_filename)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π UTF-8 —Å BOM –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –≤ Excel
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ!")
        print(f"üìä –§–∞–π–ª: {csv_path}")
        print(f"üì¶ –†–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∞ /download/
        return csv_filename

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
        raise


def convert_stats_to_json_serializable(stats):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ –æ–±—ã—á–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON"""
    import numpy as np

    result = {}
    for key, value in stats.items():
        if isinstance(value, (np.integer, np.int64, np.int32)):
            result[key] = int(value)
        elif isinstance(value, (np.floating, np.float64, np.float32)):
            result[key] = float(value)
        elif isinstance(value, list):
            result[key] = value
        else:
            result[key] = value
    return result


def process_and_upload(file_path, credentials_file='/home/vulcan4ik/dashboard-cruise-app/credentials.json'):
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (df, filename, stats)
    """
    print("\n" + "="*50)
    print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•")
    print("="*50 + "\n")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    processed_df = process_data(file_path)

    print("\n" + "="*50)
    print("üì§ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*50 + "\n")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –∏ –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ Google Sheets
    csv_filename = upload_to_sheets(processed_df, credentials_file)

    print("\n" + "="*50)
    print("‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*50 + "\n")

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º stats –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
    stats_clean = convert_stats_to_json_serializable(PROCESSING_STATS)

    print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–æ—á–∏—â–µ–Ω–Ω–∞—è): {stats_clean}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    return processed_df, csv_filename, stats_clean




